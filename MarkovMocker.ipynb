{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import random\n",
      "        \n",
      "class MarkovModel(object):\n",
      "    \"\"\"\n",
      "    Takes iterator of tokens and makes a markov model of the tokens. n is the \"order\" of the model\n",
      "    None is a special token that serves as a sort of delimiter of phrases.\n",
      "    \"\"\"\n",
      "    @classmethod\n",
      "    def _tokenizer(cls,text,token_delim):\n",
      "        for phrase in text.split(\"\\n\"):\n",
      "            for token in phrase.split(token_delim):\n",
      "                yield token\n",
      "            yield None\n",
      "    \n",
      "    @classmethod\n",
      "    def fromText(cls,text,token_delim=\".\",n=1):\n",
      "        return MarkovModel(MarkovModel._tokenizer(text,token_delim),n)\n",
      "    \n",
      "    def __init__(self,token_iterator,n=1):\n",
      "        self.n = n\n",
      "        self.model_dict = defaultdict(lambda: {\"count\":0,\"tokens_and_counts\":defaultdict(int)})\n",
      "        key = (None,) #this is a tuple\n",
      "        for token in token_iterator:\n",
      "            sub_dict = self.model_dict[key]\n",
      "            sub_dict[\"count\"] += 1\n",
      "            sub_dict[\"tokens_and_counts\"][token] += 1\n",
      "            key = self._shift_key(key,token)\n",
      "        self.model_dict.default_factory = lambda:None #make it so that you can't add anything new\n",
      "                \n",
      "    def __repr__(self):\n",
      "        string = \"\"\n",
      "        for key,counts in self.model_dict.iteritems():\n",
      "            string += \"{0}\\tcount:{1}\\n\".format(key,counts[\"count\"])\n",
      "            for token,count in counts[\"tokens_and_counts\"].iteritems():\n",
      "                string += \"\\t{0}\\tcount:{1}\\n\".format(token,count)\n",
      "        return string\n",
      "    \n",
      "    def generateSample(self,max_tokens=100):\n",
      "        key = (None,)\n",
      "        tokens = []\n",
      "        for i in xrange(max_tokens):\n",
      "            sub_dict = self.model_dict[key] \n",
      "            if sub_dict is None:\n",
      "                return tokens #here we have reached a dead end\n",
      "            until = random.randint(0,sub_dict[\"count\"])\n",
      "            for token,count in sub_dict[\"tokens_and_counts\"].iteritems():\n",
      "                until -= count\n",
      "                if until <= 0:\n",
      "                    if token is None:\n",
      "                        return tokens #here we have reached the end of a phrase\n",
      "                    tokens.append(token)\n",
      "                    key = self._shift_key(key,token)\n",
      "                    break\n",
      "        return tokens #here we have reached the max_tokens\n",
      "                \n",
      "    def _shift_key(self,key,token):\n",
      "        if token is None:\n",
      "            key = (token,)\n",
      "        else:\n",
      "            key = list(key)\n",
      "            key.append(token)\n",
      "            if len(key)>self.n:\n",
      "                del(key[0])\n",
      "            key = tuple(key)\n",
      "        return key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tweepy\n",
      "import os\n",
      "auth = tweepy.OAuthHandler(os.getenv(\"TWITTER_CONSUMER_KEY\"),os.getenv(\"TWITTER_CONSUMER_SECRET\"))\n",
      "auth.set_access_token(os.getenv(\"TWITTER_BOT_TOKEN\"), os.getenv(\"TWITTER_BOT_SECRET\"))\n",
      "t = tweepy.API(auth)\n",
      "\n",
      "def make_fun_of(screen_name,n=1):\n",
      "    ms=t.user_timeline( screen_name=screen_name,count=200)\n",
      "    text = []\n",
      "    for m in ms:\n",
      "        text.extend(m.text.split(\" \"))\n",
      "        text.append(None)\n",
      "    mm = MarkovModel(text,n=n)\n",
      "    return mm\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preetam = make_fun_of(\"PreetamJinka\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(preetam.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@JnBrymn @usnishm\n"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jnbrymn = make_fun_of(\"jnbrymn\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 158
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(jnbrymn.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".@VividCortex is International Girls in print on a valid use of these new reign as n approaches infinity, no?\n"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "superninjarobot = make_fun_of(\"superninjarobot\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(superninjarobot.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT @uphexco: Big thanks to load dishwashers efficiently that have an infinitely growing chocolate bar. http://t.co/U5H4sZxsl5\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "softwaredoug = make_fun_of(\"softwaredoug\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(softwaredoug.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@HumongoDB demoralization is of-course nice voucher for excel therefore I certainty do!\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "amontalenti = make_fun_of(\"amontalenti\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(amontalenti.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RT @MinaIlhan: The Totem Pole: \"Each role of life's totem pole, just need modular teams just *think* they're sworn enemies\n"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thefoodgeek = make_fun_of(\"thefoodgeek\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(thefoodgeek.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@AngerMonkey @mikesphar That would usually have a 50% discount across the \u2018whee!\u2019 in the Australian accent, I\u2019ll just like labeling things I know. Check eBay, you need it, I thought you the podcasting.\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jeffspies = make_fun_of(\"jeffspies\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(jeffspies.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@epistemographer @cboettig @github and that's an open PR policy, quick to go to announce release of the browser?  http://t\u2026\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "juphoff = make_fun_of(\"juphoff\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bbombgardener = make_fun_of(\"bbombgardener\")\n",
      "print \" \".join(bbombgardener.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@amontalenti Oh, great! Glad I enjoy living.\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \" \".join(bbombgardener.generateSample())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@brendengrace Did you can do: record the language. Definitely lines with all www.did[team/city]http://t.co/euOA5M7MYS and OJ. Life is trending on SO for this! I can't be interesting to the hour on # plausible games of the NCAA tournament to the ACC championship? I'm always amazed at the #Heartbleed Bug for an entry every day.\n"
       ]
      }
     ],
     "prompt_number": 151
    }
   ],
   "metadata": {}
  }
 ]
}